{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import set_config ## SE AGREGA ESTO PARA QUE EL DIAGRAMA DEL PIPELINE APAREZCA INTERACTIVO\n",
    "set_config(display = 'diagram') ## SE AGREGA ESTO PARA QUE EL DIAGRAMA DEL PIPELINE APAREZCA INTERACTIVO\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.metrics import roc_auc_score, cohen_kappa_score, accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "from  sklearn.base import clone ## Copiar un pipeline\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Realizamos una copia del dataset original en caso de volver a ocupar esta versión sin cambios.\n",
    "train_original = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se verifica si hay datos duplicados\n",
    "train.loc[train.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se elimina la variable ID, no es relevante para el análisis\n",
    "train.drop('ID', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza y EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Control de calidad de los datos.\n",
    "En esta sección se realiza una revisión del tipo de variables y el número de datos faltantes en el data set de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = train.isnull().sum()[train.isnull().sum()>0].sort_values().plot(kind = 'barh')\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax.set_title('Variables con valores nulos')\n",
    "ax.get_xaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum(axis = 1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En el control de calidad de los datos, se encuentra que el dataset consta de 8.608 observaciones y 11 variables, las cuales todas tienen el tipo de variable esperado. \n",
    "\n",
    "* Con respecto a los datos faltantes, la variable ***Work_Experience*** es la que mayor cantidad de nulos tiene (829), seguido de ***Family_Size*** (335).\n",
    "\n",
    "* En cuanto a el número de nulos por observación se encuentra que, existen 6.665 registros completos, 1.244 registros con un dato faltante en 1 columna, 140 registros con datos faltantes en 2 columnas y 19 registros con al menos 3 columnas sin dato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SE DEFINE FUNCIÓN PARA SEPARAR LAS VARIABLES CATEGÓRICAS, NUMÉRICAS Y LA DEPENDIENTE\n",
    "def SepararNumCate(df : pd.DataFrame):\n",
    "    '''Returns a triplet with column names (numerical, categorical, target)\n",
    "    '''\n",
    "    numerical = df.select_dtypes(include = 'number').columns.to_list()\n",
    "    categorical = df.select_dtypes(exclude = 'number').columns.to_list()\n",
    "    categorical.remove('Segmentation') ## REMOVES THE TARGET VARIABLE\n",
    "    target = ['Segmentation']\n",
    "    return numerical, categorical, target\n",
    "\n",
    "\n",
    "num_idx, categ_idx, target_idx = SepararNumCate(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorías de la variable \"Gender\"  ['Male' 'Female'] \n",
      "\n",
      "Categorías de la variable \"Ever_Married\"  ['No' 'Yes' nan] \n",
      "\n",
      "Categorías de la variable \"Graduated\"  ['No' 'Yes' nan] \n",
      "\n",
      "Categorías de la variable \"Profession\"  ['Healthcare' 'Engineer' 'Lawyer' 'Entertainment' 'Artist' 'Executive'\n",
      " 'Doctor' 'Homemaker' 'Marketing' nan] \n",
      "\n",
      "Categorías de la variable \"Spending_Score\"  ['Low' 'Average' 'High'] \n",
      "\n",
      "Categorías de la variable \"Var_1\"  ['Cat_4' 'Cat_6' 'Cat_7' 'Cat_3' 'Cat_1' 'Cat_2' nan 'Cat_5'] \n",
      "\n",
      "Categorías de la variable \"Segmentation\"  ['D' 'A' 'B' 'C'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Identificar los valores únicos de las variables categóricas\n",
    "for i in train[categ_idx + target_idx].columns:\n",
    "    print(f'Categorías de la variable \"{i}\" ',train[i].unique(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para elegir qué se va a realizar con respecto a los valores faltantes se proponen 3 estrategias que se pondrán a prueba:\n",
    "\n",
    "> - 1. Un data set de entrenamiento eliminando todos las observaciones que tengan al menos 1 dato faltante, lo que representa quedarse con un 75% del data set de entrenamiento inicial.\n",
    "\n",
    "> - 2. Un data set de entrenamiento donde se eliminan las observaciones que tienen 2 o más datos nulos y a la observación que quede con dato un nulo se le asigna el valor del dato, mediante algún método de imputación.\n",
    "\n",
    "> - 3. Un data set donde no se elimina ningún registro, se asigna el valor de los datos por algún método de imputación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definimos los pipelines y la función por la cual va a pasar cada uno de los dataset de las anteriores estrategias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipelines\n",
    "\n",
    "numerical_pipeline = Pipeline( steps = [\n",
    "    ('scaler',StandardScaler())\n",
    "])\n",
    "\n",
    "categ_pipeline = Pipeline( steps = [\n",
    "    ('encoder',OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "## ColumnTransformer\n",
    "columnsTransf = ColumnTransformer(transformers = [\n",
    "                    ('numerical_pl',numerical_pipeline,num_idx),\n",
    "                    ('categorical_pl',categ_pipeline,categ_idx)\n",
    "                            ],\n",
    "                    remainder = 'drop'\n",
    "                                )\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state = 123)\n",
    "lr_model = LogisticRegression(random_state = 123)\n",
    "nb_model = GaussianNB()\n",
    "kn_clf_model = KNeighborsClassifier()\n",
    "xgb_clf = xgb.XGBClassifier(random_state = 123)\n",
    "\n",
    "\n",
    "### Diccionario que contiene todos los pipelines\n",
    "pl_dict = { 'RandomForest' : Pipeline( steps = [\n",
    "                                    ('col_transf',columnsTransf),\n",
    "                                    ('model',rf_model)\n",
    "                                        ]),\n",
    "            'LogisticRegression' : Pipeline( steps = [\n",
    "                                    ('col_transf',columnsTransf),\n",
    "                                    ('model',lr_model)\n",
    "                                        ]),\n",
    "            'NaiveBayes' : Pipeline( steps = [\n",
    "                                    ('col_transf',columnsTransf),\n",
    "                                    ('model',nb_model)\n",
    "                                         ]),\n",
    "            'KNeighborsClassifier' : Pipeline( steps = [\n",
    "                                    ('col_transf',columnsTransf),\n",
    "                                    ('model',kn_clf_model)\n",
    "                                         ]),\n",
    "            'XGBClassifier' : Pipeline( steps = [\n",
    "                                    ('col_transf',columnsTransf),\n",
    "                                    ('model',xgb_clf)\n",
    "                                         ])                                         \n",
    "           }\n",
    "\n",
    "## Crear el pipeline para la imputación\n",
    "num_imputer = Pipeline(steps = [\n",
    "        ('num_imputer',SimpleImputer(strategy = 'median')),\n",
    "])\n",
    "\n",
    "cat_imputer = Pipeline(steps = [\n",
    "    ('cat_imputer',SimpleImputer(strategy = 'most_frequent'))\n",
    "])\n",
    "\n",
    "columnImputer = ColumnTransformer(transformers = [\n",
    "    ('Numerical',num_imputer,num_idx),\n",
    "    ('Categorical', cat_imputer, categ_idx)\n",
    "                                ], \n",
    "            remainder = 'passthrough' ## En este caso no se utiliza el passthrough porque necesitamos nuestra variable objetivo, ya que este pipeline es sólo para transformar columna\n",
    ")\n",
    "\n",
    "imputer_pipe = Pipeline(steps=[\n",
    "    ('columns_transf',columnImputer)\n",
    "])\n",
    "\n",
    "def ImputersTest(df : pd.DataFrame,  numerical_columns : list, \n",
    "                 categorical_columns : list, target : str, model : str): \n",
    "    if model in pl_dict.keys():\n",
    "        pipe = clone(pl_dict.get(model))\n",
    "        print(f\"Accuracy with CV : {cross_val_score(pipe,df[numerical_columns + categorical_columns], df[target], cv = 5, scoring = 'accuracy').mean()}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f'{model} no es un modelo implementado, las opciones son {list(pl_dict.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hacer ningún tratamiento de imputación, se eliminan las filas que contengan NA's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train.dropna( axis = 0).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with CV : 0.48012003000750186\n"
     ]
    }
   ],
   "source": [
    "ImputersTest(train1 ,num_idx, categ_idx, target_idx, 'RandomForest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se eliminan las filas que tienen datos faltantes en 2 o más de sus columnas\n",
    "train2 = train.drop(train.loc[train.isna().sum(axis = 1) >= 2].index).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_imp = pd.DataFrame(clone(imputer_pipe).fit_transform(train2), columns = num_idx + categ_idx + target_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with CV : 0.48173082535897604\n"
     ]
    }
   ],
   "source": [
    "ImputersTest(train2_imp,num_idx,categ_idx,target_idx,'RandomForest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3 = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3_imp = pd.DataFrame(clone(imputer_pipe).fit_transform(train3), columns = num_idx + categ_idx + target_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with CV : 0.476822763620552\n"
     ]
    }
   ],
   "source": [
    "ImputersTest(train3_imp,num_idx,categ_idx,target_idx,'RandomForest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De las 3 estrategias propuestas y evaluadas, todas presentaron un accuracy similar, No obstante si se eliminan todos los datos faltantes (estrategia 1) se perdería alrededor del 17% de las observaciones, mientras que si se eliminan sólo aquellos registros cuyas observaciones tengan nulos en 2 o más variables se perdería alrededor del 2% del dataset original. Por lo cual, se realizará el resto del ejercicio con el dataset derivado de la estrategia 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definimos el dataset imputado para seguir trabajando el resto del ejercicio\n",
    "del train ## se elimina el df que tenía este nombre\n",
    "\n",
    "train = train2_imp.copy().astype(train1.dtypes.to_dict()) ## Se le asignan el tipo de variables de uno de los datasets ya que con la imputación todas las variables quedan como tipo object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Visualización de la información"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Balanceo de la variable dependiente**\n",
    "\n",
    "  Cada una de las categorías de la variable Segmentation contiene alrededor del 25% de las observaciones, por lo que al dataset no hay que realizarle procesos de balanceo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = train.Segmentation.value_counts(normalize = True).sort_index().plot( kind = 'bar')\n",
    "ax.get_yaxis().set_visible(False)\n",
    "labels = (train.Segmentation.value_counts(normalize = True).sort_index()*100).round(1).astype('str') + '%' ## ETIQUETA DE LAS BARRAS QUE SE MUESTRE EN %\n",
    "ax.tick_params(axis='x', rotation = 0)  ## ROTAR LAS ETIQUETAS DEL EJE X\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, labels = labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DESCRIPCIÓN VARIABLES NUMÉRICAS VS VARIABLE DEPENDIENTE\n",
    "* **Segmentación vs Edad:**\n",
    "\n",
    "    En los siguientes gráficos se observa que el grupo D, tiene en promedio una edad menor que el resto de grupos, siendo esta alrededor de los 32 años. Los 3 grupos restantes tienen una edad promedio muy cercana a los 50 años.\n",
    "\n",
    "* **Segmentación vs Experiencia laboral**\n",
    "\n",
    "    En la gráfica siguiente se observa que el promedio de experiencia laboral para todas las categorías de Segmentation se encuentra entre los 2 y 3 años, también es posible identificar que el grupo D es el que más experiencia tiene puesto que un 75% de los individuos tienen una experiencia laboral de 6 años o menos, y el grupo C es el que menos experiencia tiene ya que el 75% de los individuos tienen una experiencia de 3 años o menos.\n",
    "\n",
    "\n",
    "* **Segmentation vs Family Size**\n",
    "\n",
    "  El tamaño del núcleo familiar tiene una distribución similar para los segmentos B, C y D, donde el 75% de los individuos cada uno de estos grupos tiene un núcleo familiar conformado por 4 o menos personas, por otra parte el núcleo familiar del Segmento A es más reducido donde el 75% de los individuos tienen un núcleo familiar de 3 o menos personas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1 ,ax2, ax3) = plt.subplots(1,3, figsize = (14,4))\n",
    "sns.boxplot(data = train, x = 'Segmentation', y = 'Age',order = train.Segmentation.sort_values().unique(), ax = ax1, showmeans = True, palette = 'vlag')\n",
    "sns.boxplot(data = train, x = 'Segmentation', y = 'Work_Experience', order = train.Segmentation.sort_values().unique(), ax = ax2, showmeans = True,palette=\"vlag\")\n",
    "sns.boxplot(data = train, x = 'Segmentation', y = 'Family_Size', order = train.Segmentation.sort_values().unique(), ax = ax3, showmeans = True, palette=\"vlag\")\n",
    "\n",
    "#//TODO ...poner la media como texto en cada uno de los boxplots\n",
    "# Calcular la media por categoría\n",
    "# means = train.groupby('Segmentation')[['Age', 'Work_Experience', 'Family_Size']].mean().values\n",
    "\n",
    "# Agregar líneas para la media y la mediana\n",
    "#for i in range(len(train.Segmentation.sort_values().unique())):\n",
    " #   plt.text(i, means[i], f'{means[i]:.2f}', ha='center', va='bottom', color='blue', fontsize=8)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Género vs Segmentación**\n",
    "\n",
    "   Con respecto a la distribución del género por Segmentación, la participación en todos los grupos de ambos géneros es similar a excepción del grupo D, donde la participación de los hombres es casi del 60%.\n",
    "\n",
    "* **Ever_Married vs Segmentación**\n",
    "  \n",
    "  La variable categórica que toma en cuenta si el individuo ha estado casado alguna vez presenta la siguiente distribución: en los grupos A, B y C la mayoría de personas han estado casados, teniendo una mayor participación elos grupos A y B alcanzando casi el 80%. El grupo D por el contrario más del 70% de los individuos no han estado casados, este grupo como se vió en anteriores gráficas es el grupo con menor promedio de edad.\n",
    "  \n",
    "* **Graduated vs Segmentation**\n",
    "\n",
    "    La variable binaria que captura si el individuo se graduó sigue el comportamiento siguiente diferenciándola por la Segmentación: En los grupos A, B y C más del 60% son graduados, caso contrario en el grupo D donde poco más del 60% de los individuos no son graduados.\n",
    "\n",
    "* **Spending_Score vs Segmentation**\n",
    "    La variable categórica que captura si el puntaje de consumo es bajo, medio o alto presenta la siguiente distribución: Los grupos A, B y D predominan las personas que tienen un consumo bajo, siendo en el grupo D casi la totalidad de personas que tienen un consumo bajo, por su parte en el grupo C  predomina el consumo promedio con un poco más del 40%.\n",
    "\n",
    "* **Profession vs Segmentation**\n",
    "  \n",
    "  Con respecto a la profesión del cliente:\n",
    "    - El grupo A la mayor participación la tienen los artistas con casi el 30% de los individuos seguido de los profesionales del entretenimiento.\n",
    "    - El grupo B la mayor participación la tienen los artistas con más del 40% de los individuos seguido por los profesionales del entretenimiento.\n",
    "    - El grupo C la mayor participación la tienen los artistas con más del 50% de los individuos seguido por los ejecutivos.\n",
    "    - El grupo D la mayor participación la tienen los profesionales de la salud con más del 40% seguido por los doctores y profesionales del entretenamiento.\n",
    "     \n",
    "* **Var_1 vs Segmentation**\n",
    "\n",
    "  Con respecto a una categoría anónima que se le asigna a cada cliente la categoría 6 tiene la mayor participación en todos los segmentos con más del 60% de todos los individuos para los grupos A, B y C, y más del 50% en el grupo D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2),\n",
    "      (ax3, ax4),\n",
    "      (ax5, ax6)) = plt.subplots(3,2, figsize = (18,12), sharey = True)\n",
    "pd.crosstab(train.Segmentation,train.Gender,train.Age,aggfunc='count', normalize = 'index').plot(kind = 'bar', ax = ax1, edgecolor = 'black',colormap= 'vlag')\n",
    "ax1.get_yaxis().set_major_formatter(matplotlib.ticker.PercentFormatter(xmax = 1,decimals = 0))  ## MUESTRA LOS TICKS DEL EJE Y EN PORCENTAJE\n",
    "ax1.tick_params(axis='x', rotation=0)  ## ROTAR LAS ETIQUETAS DEL EJE X\n",
    "ax1.legend(loc = 'upper left', title = 'Gender')\n",
    "ax1.set_xlabel(None)\n",
    "\n",
    "pd.crosstab(train.Segmentation,train.Ever_Married,train.Age,aggfunc='count', normalize = 'index').plot(kind = 'bar', ax = ax2, edgecolor = 'black', colormap= 'vlag')\n",
    "ax2.get_yaxis().set_major_formatter(matplotlib.ticker.PercentFormatter(xmax = 1,decimals = 0))  ## MUESTRA LOS TICKS DEL EJE Y EN PORCENTAJE\n",
    "ax2.tick_params(axis='x', rotation=0) \n",
    "ax2.legend(loc = 'upper left',  title = 'Gender')\n",
    "ax2.set_xlabel(None)\n",
    "\n",
    "pd.crosstab(train.Segmentation,train.Graduated,train.Age,aggfunc='count', normalize = 'index').plot(kind = 'bar', ax = ax3, edgecolor = 'black',colormap= 'vlag')\n",
    "ax3.get_yaxis().set_major_formatter(matplotlib.ticker.PercentFormatter(xmax = 1,decimals = 0))  ## MUESTRA LOS TICKS DEL EJE Y EN PORCENTAJE\n",
    "ax3.tick_params(axis='x', rotation=0)  ## ROTAR LAS ETIQUETAS DEL EJE X\n",
    "ax3.legend(loc = 'upper left', title = 'Ever_Married')\n",
    "ax3.set_xlabel(None)\n",
    "\n",
    "pd.crosstab(train.Segmentation,train.Spending_Score,train.Age,aggfunc='count', normalize = 'index').plot(kind = 'bar', ax = ax4, edgecolor = 'black',colormap= 'vlag')\n",
    "ax4.get_yaxis().set_major_formatter(matplotlib.ticker.PercentFormatter(xmax = 1,decimals = 0))  ## MUESTRA LOS TICKS DEL EJE Y EN PORCENTAJE\n",
    "ax4.tick_params(axis='x', rotation = 0) \n",
    "ax4.legend(loc = 'upper left', title = 'Spending_Score')\n",
    "ax4.set_xlabel(None)\n",
    "\n",
    "pd.crosstab(train.Segmentation,train.Profession,train.Age,aggfunc='count', normalize = 'index').plot(kind = 'bar', ax = ax5, edgecolor = 'black',colormap= 'vlag')\n",
    "ax5.get_yaxis().set_major_formatter(matplotlib.ticker.PercentFormatter(xmax = 1,decimals = 0))  ## MUESTRA LOS TICKS DEL EJE Y EN PORCENTAJE\n",
    "ax5.tick_params(axis='x', rotation=0)  ## ROTAR LAS ETIQUETAS DEL EJE X\n",
    "ax5.legend(loc = 'upper left', ncols = 3, title = 'Profession')\n",
    "ax5.set_xlabel(None)\n",
    "\n",
    "pd.crosstab(train.Segmentation,train.Var_1,train.Age,aggfunc='count', normalize = 'index').plot(kind = 'bar', ax = ax6, edgecolor = 'black',colormap= 'vlag')\n",
    "ax6.get_yaxis().set_major_formatter(matplotlib.ticker.PercentFormatter(xmax = 1,decimals = 0))  ## MUESTRA LOS TICKS DEL EJE Y EN PORCENTAJE\n",
    "ax6.tick_params(axis='x', rotation=0) \n",
    "ax6.legend(loc = 'upper left', ncols = 3)\n",
    "ax6.set_xlabel(None)\n",
    "fig.suptitle('Variables categóricas vs variable objetivo', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_coeficiente_contingencia(df: pd.DataFrame, cols: list, target_col: str) -> pd.DataFrame:\n",
    "    resultados = []\n",
    "    for col in cols:\n",
    "        tabla_contingencia = pd.crosstab(df[col], df[target_col])\n",
    "        chi2, p, dof, expected = chi2_contingency(tabla_contingencia)\n",
    "        n = tabla_contingencia.values.sum()\n",
    "        coef_contingencia = np.sqrt(chi2 / (n * min(len(tabla_contingencia.index) - 1, len(tabla_contingencia.columns) - 1)))\n",
    "        resultados.append([col, coef_contingencia])\n",
    "    df_resultado = pd.DataFrame(resultados, columns=['Variable', 'Coeficiente de Contingencia'])\n",
    "    df_resultado.sort_values(by='Coeficiente de Contingencia', ascending=False, inplace=True)\n",
    "    df_resultado.reset_index(drop=True, inplace=True)\n",
    "    return df_resultado.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretación asociación entre variables categóricas independientes y variable objetivo**\n",
    " - Todas las variables tienen un coeficiente menor a 0.6, por lo que ninguna presenta una asociación relativamente intensa con la variable objetivo. Sin embargo, la variable Ever_Married es la que mayor coeficiente de asociación tiene.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcular_coeficiente_contingencia(train1,categ_idx, 'Segmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Extracción de información de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Comprensión y limpieza de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  //TODO ...SE DEBERÍAN ELIMINAR LOS OUTLIERS  ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Entrenar el modelo de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definir protocolos de evaluación\n",
    "def ClfScores(model, x_train,y_train,x_test,y_test):\n",
    "    '''\n",
    "    model : must be a previously fitted estimator\n",
    "    '''\n",
    "    yTest_pred = model.predict(x_test)\n",
    "    yTrain_pred = model.predict(x_train)\n",
    "    acc_test = accuracy_score(y_test, yTest_pred)\n",
    "    acc_train = accuracy_score(y_train, yTrain_pred)\n",
    "    ck_train = cohen_kappa_score(y_train, yTrain_pred)\n",
    "    ck_test = cohen_kappa_score(y_test, yTest_pred)\n",
    "    precision_train = precision_score(y_train, yTrain_pred, average = None)\n",
    "    precision_test = precision_score(y_test, yTest_pred, average = None)\n",
    "    rec_train = recall_score(y_train, yTrain_pred, average = None)\n",
    "    rec_test = recall_score(y_test, yTest_pred, average = None)\n",
    "    metrics = {\n",
    "                'Accuracy test' : acc_test,\n",
    "                'Accuracy train' : acc_train,\n",
    "                'Cohen Kappa train' :  ck_train,\n",
    "                'Cohen Kappa test' :  ck_test,\n",
    "                'Precision train' : precision_train,\n",
    "                'Precision test' : precision_test,\n",
    "                'Recall train' :  rec_train,\n",
    "                'Recall test'  :  rec_test\n",
    "    }\n",
    "    for i in metrics.keys():\n",
    "        print(i,' : ', metrics.get(i))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HoldOut\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[num_idx + categ_idx], train[target_idx], test_size = 0.2, random_state = 123)\n",
    "\n",
    "target_transf = LabelEncoder()  ## Es necesario etiquetar la variable objetivo, algunos algoritmos lo requieren (XGBClassifier)\n",
    "y_train = target_transf.fit_transform(y_train)\n",
    "y_test = target_transf.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DummyClassifier(strategy=&#x27;most_frequent&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DummyClassifier</label><div class=\"sk-toggleable__content\"><pre>DummyClassifier(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DummyClassifier(strategy='most_frequent')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy = 'most_frequent')\n",
    "dummy_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test  :  0.2686472819216182\n",
      "Accuracy train  :  0.2803856488067014\n",
      "Cohen Kappa train  :  0.0\n",
      "Cohen Kappa test  :  0.0\n",
      "Precision train  :  [0.         0.         0.         0.28038565]\n",
      "Precision test  :  [0.         0.         0.         0.26864728]\n",
      "Recall train  :  [0. 0. 0. 1.]\n",
      "Recall test  :  [0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "ClfScores(dummy_clf,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipelines modelos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid parameters for each of the models\n",
    "rf_grid_params = { 'model__max_depth': [1,4,8,12],\n",
    "                'model__max_features': ['sqrt'],\n",
    "                'model__max_leaf_nodes': [2,4,6,8],\n",
    "                'model__min_samples_leaf': [5,10,20],\n",
    "                'model__min_samples_split': [2, 5, 10],\n",
    "                'model__n_estimators': [100,200]}\n",
    "\n",
    "nb_grid_params = {'model__var_smoothing': np.logspace(0,-9, num = 100)}\n",
    "\n",
    "kn_grid_params = {'model__n_neighbors' : np.arange(1,30)}\n",
    "\n",
    "xgb_grid_params = {'model__learning_rate': [0.03,0.3],\n",
    "                  'model__subsample'    : [1, 0.5, 0.2],\n",
    "                  'model__n_estimators' : [100,500,1000],\n",
    "                  'model__max_depth'    : [None,5,7,12] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test  :  0.4936788874841972\n",
      "Accuracy train  :  0.510826616089774\n",
      "Cohen Kappa train  :  0.3425257985244785\n",
      "Cohen Kappa test  :  0.3224103966992705\n",
      "Precision train  :  [0.41510695 0.40031153 0.51292247 0.60725769]\n",
      "Precision test  :  [0.43342776 0.36129032 0.49494949 0.56476684]\n",
      "Recall train  :  [0.40403383 0.17760885 0.65774379 0.74520857]\n",
      "Recall test  :  [0.38345865 0.14698163 0.64986737 0.76941176]\n"
     ]
    }
   ],
   "source": [
    "## RandomForest\n",
    "rf_gs = GridSearchCV(clone(pl_dict.get('RandomForest')), rf_grid_params, n_jobs = -4, cv = 5, scoring = 'accuracy')\n",
    "rf_gs.fit(X_train, y_train)\n",
    "rf_model = clone(pl_dict.get('RandomForest')).set_params(**rf_gs.best_params_)\n",
    "rf_model.fit(X_train, y_train)\n",
    "ClfScores(rf_model,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test  :  0.5139064475347661\n",
      "Accuracy train  :  0.5158843053579896\n",
      "Cohen Kappa train  :  0.3510956530147953\n",
      "Cohen Kappa test  :  0.3503694147248987\n",
      "Precision train  :  [0.41864407 0.40755208 0.51560021 0.65068493]\n",
      "Precision test  :  [0.43678161 0.40104167 0.50425532 0.6371134 ]\n",
      "Recall train  :  [0.482108   0.21630961 0.62141491 0.69616685]\n",
      "Recall test  :  [0.47619048 0.20209974 0.62864721 0.72705882]\n"
     ]
    }
   ],
   "source": [
    "## LogisticRegression\n",
    "lr_model = clone(pl_dict.get('LogisticRegression'))\n",
    "lr_model.fit(X_train, y_train)\n",
    "ClfScores(lr_model,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test  :  0.4949431099873578\n",
      "Accuracy train  :  0.49865655128813025\n",
      "Cohen Kappa train  :  0.32722667696659846\n",
      "Cohen Kappa test  :  0.32554627839174954\n",
      "Precision train  :  [0.44932432 0.38042131 0.46065013 0.62012012]\n",
      "Precision test  :  [0.48122867 0.39215686 0.45053004 0.59152216]\n",
      "Recall train  :  [0.34612882 0.2121631  0.68642447 0.69842165]\n",
      "Recall test  :  [0.35338346 0.20997375 0.67639257 0.72235294]\n"
     ]
    }
   ],
   "source": [
    "## NaiveBayes\n",
    "nb_gs = GridSearchCV(clone(pl_dict.get('NaiveBayes')), nb_grid_params, n_jobs = -4, cv = 5, scoring = 'accuracy')\n",
    "nb_gs.fit(X_train, y_train)\n",
    "nb_model = clone(pl_dict.get('NaiveBayes')).set_params(**nb_gs.best_params_)\n",
    "nb_model.fit(X_train, y_train)\n",
    "ClfScores(nb_model,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test  :  0.5075853350189633\n",
      "Accuracy train  :  0.5509720246562352\n",
      "Cohen Kappa train  :  0.3988212352243863\n",
      "Cohen Kappa test  :  0.34210647132367944\n",
      "Precision train  :  [0.48389218 0.44579358 0.56030006 0.65885417]\n",
      "Precision test  :  [0.44390244 0.37984496 0.52764977 0.6125    ]\n",
      "Recall train  :  [0.47885491 0.35521769 0.61886552 0.71307779]\n",
      "Recall test  :  [0.45614035 0.25721785 0.60742706 0.69176471]\n"
     ]
    }
   ],
   "source": [
    "## KNeighborsClassifier\n",
    "kn_gs = GridSearchCV(clone(pl_dict.get('KNeighborsClassifier')), kn_grid_params, n_jobs = -4, cv = 5, scoring = 'accuracy')\n",
    "kn_gs.fit(X_train, y_train)\n",
    "kn_model = clone(pl_dict.get('KNeighborsClassifier')).set_params(**kn_gs.best_params_)\n",
    "kn_model.fit(X_train, y_train)\n",
    "ClfScores(kn_model,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test  :  0.5372945638432364\n",
      "Accuracy train  :  0.6108740319266635\n",
      "Cohen Kappa train  :  0.47835166338454294\n",
      "Cohen Kappa test  :  0.3811700026505467\n",
      "Precision train  :  [0.54285714 0.56262231 0.62348668 0.6784141 ]\n",
      "Precision test  :  [0.48129676 0.41791045 0.56153846 0.62332696]\n",
      "Recall train  :  [0.56864021 0.39737388 0.65646909 0.78128523]\n",
      "Recall test  :  [0.48370927 0.29396325 0.58090186 0.76705882]\n"
     ]
    }
   ],
   "source": [
    "## XGBClassifier\n",
    "xgb_gs = GridSearchCV(clone(pl_dict.get('XGBClassifier')), xgb_grid_params, n_jobs = -4, cv = 5, scoring = 'accuracy')\n",
    "xgb_gs.fit(X_train, y_train)\n",
    "xgb_model = clone(pl_dict.get('XGBClassifier')).set_params(**xgb_gs.best_params_)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "ClfScores(xgb_model,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducción dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caracterización de los clientes por segmento"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
